{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kerasgan.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjkpeZsPJB/dcTqjIIycUr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreigFodd/Network-Data-Generation-Vanilla-GAN/blob/main/kerasgan_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhBFUkghmHwI",
        "outputId": "86e01484-6557-4e6f-994b-f072f218b103"
      },
      "source": [
        "# link & mount to google drive to import data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SKCxa_hmT6W"
      },
      "source": [
        "# import libs\n",
        "import pandas as pd\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U78VRsFlASzF"
      },
      "source": [
        "**Load in data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOtjhXihLxkU"
      },
      "source": [
        "# import pre-processed data from google drive and store in dataframe using pandas function\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab/ProjectData/data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyvMikaPIgiu",
        "outputId": "35cfbb91-a26b-4745-dc9f-1aedf4e4dc1b"
      },
      "source": [
        "# print columns\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['init_win_bytes_fwd', 'subflow_fwd_bytes',\n",
              "       'total_length_of_fwd_packets', 'label', 'fwd_packet_length_std',\n",
              "       'bwd_packet_length_std', 'flow_iat_mean', 'fwd_iat_min', 'flow_iat_std',\n",
              "       'flow_duration', 'active_mean', 'fwd_psh_flags', 'syn_flag_count',\n",
              "       'fwd_packets_s'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq_x6qHY_7TM",
        "outputId": "a92625a0-9cb7-4ade-9797-088bc24a8c8c"
      },
      "source": [
        "# ensure correct data was loaded\n",
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN           1170495\n",
              "DoS Hulk           70302\n",
              "SSH-Patator         2922\n",
              "DoS slowloris       2332\n",
              "Heartbleed            11\n",
              "Infiltration           4\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOD4sduP_CUU"
      },
      "source": [
        "# split the dataset into 6 different datasets, one for each class\n",
        "# GAN model will generate samples for one class at a time, therefore dataframe needs to be split by class. Will be joined again after all samples have been generated. \n",
        "\n",
        "df_benign = df.loc[df['label']=='BENIGN']\n",
        "df_dosHulk = df.loc[df['label']=='DoS Hulk']\n",
        "df_sshPatator = df.loc[df['label']=='SSH-Patator']\n",
        "df_dosSlowloris = df.loc[df['label']=='DoS slowloris']\n",
        "df_heartbleed = df.loc[df['label']=='Heartbleed']\n",
        "df_infiltration = df.loc[df['label']=='Infiltration']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3av6uTS_AXPk"
      },
      "source": [
        "**Drop label category from each dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioMwGQt8BHFn"
      },
      "source": [
        "df_benign = df_benign.drop(['label'], axis=1)\n",
        "df_dosHulk = df_dosHulk.drop(['label'], axis=1)\n",
        "df_sshPatator = df_sshPatator.drop(['label'], axis=1)\n",
        "df_dosSlowloris = df_dosSlowloris.drop(['label'], axis=1)\n",
        "df_heartbleed = df_heartbleed.drop(['label'], axis=1)\n",
        "df_infiltration = df_infiltration.drop(['label'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvd1LnbzK62l"
      },
      "source": [
        "**Apply power transformer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYzAHi0qL2qG"
      },
      "source": [
        "# Power transformer || https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html\n",
        "# makes data more 'Gaussian-like' \n",
        "\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "pw= PowerTransformer(method='yeo-johnson', standardize=True, copy=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7j9XcnUDWHA",
        "outputId": "9d8c8519-bd00-49c8-9ed7-24c07945a0d2"
      },
      "source": [
        "pwt_benign = pw.fit_transform(df_benign[df_benign.columns])\n",
        "pwt_dosHulk = pw.fit_transform(df_dosHulk[df_dosHulk.columns])\n",
        "pwt_sshPatator = pw.fit_transform(df_sshPatator[df_sshPatator.columns])\n",
        "pwt_dosSlowloris = pw.fit_transform(df_dosSlowloris[df_dosSlowloris.columns])\n",
        "pwt_heartbleed = pw.fit_transform(df_heartbleed[df_heartbleed.columns])\n",
        "pwt_infiltration = pw.fit_transform(df_infiltration[df_infiltration.columns])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:2982: RuntimeWarning: divide by zero encountered in log\n",
            "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:2982: RuntimeWarning: divide by zero encountered in log\n",
            "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:2982: RuntimeWarning: divide by zero encountered in log\n",
            "  loglike = -n_samples / 2 * np.log(x_trans.var())\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:2982: RuntimeWarning: divide by zero encountered in log\n",
            "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRSo3m2sIFhC"
      },
      "source": [
        "df_pwt_benign = pd.DataFrame(pwt_benign)\n",
        "df_pwt_dosHulk = pd.DataFrame(pwt_dosHulk)\n",
        "df_pwt_sshPatator = pd.DataFrame(pwt_sshPatator)\n",
        "df_pwt_dosSlowloris = pd.DataFrame(pwt_dosSlowloris)\n",
        "df_pwt_heartbleed = pd.DataFrame(pwt_heartbleed)\n",
        "df_pwt_infiltration = pd.DataFrame(pwt_infiltration)\n",
        "\n",
        "#df_pwt_benign.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKnXMA1OIbbN"
      },
      "source": [
        "df_pwt_benign.columns = ['init_win_bytes_fwd', 'subflow_fwd_bytes',\n",
        "       'total_length_of_fwd_packets', 'fwd_packet_length_std',\n",
        "       'bwd_packet_length_std', 'flow_iat_mean', 'fwd_iat_min', 'flow_iat_std',\n",
        "       'flow_duration', 'active_mean', 'fwd_psh_flags', 'syn_flag_count',\n",
        "       'fwd_packets_s']\n",
        "\n",
        "df_pwt_dosHulk.columns = ['init_win_bytes_fwd', 'subflow_fwd_bytes',\n",
        "       'total_length_of_fwd_packets', 'fwd_packet_length_std',\n",
        "       'bwd_packet_length_std', 'flow_iat_mean', 'fwd_iat_min', 'flow_iat_std',\n",
        "       'flow_duration', 'active_mean', 'fwd_psh_flags', 'syn_flag_count',\n",
        "       'fwd_packets_s'] \n",
        "\n",
        "df_pwt_sshPatator.columns = ['init_win_bytes_fwd', 'subflow_fwd_bytes',\n",
        "       'total_length_of_fwd_packets', 'fwd_packet_length_std',\n",
        "       'bwd_packet_length_std', 'flow_iat_mean', 'fwd_iat_min', 'flow_iat_std',\n",
        "       'flow_duration', 'active_mean', 'fwd_psh_flags', 'syn_flag_count',\n",
        "       'fwd_packets_s']\n",
        "\n",
        "df_pwt_dosSlowloris.columns = ['init_win_bytes_fwd', 'subflow_fwd_bytes',\n",
        "       'total_length_of_fwd_packets', 'fwd_packet_length_std',\n",
        "       'bwd_packet_length_std', 'flow_iat_mean', 'fwd_iat_min', 'flow_iat_std',\n",
        "       'flow_duration', 'active_mean', 'fwd_psh_flags', 'syn_flag_count',\n",
        "       'fwd_packets_s']\n",
        "\n",
        "df_pwt_heartbleed.columns = ['init_win_bytes_fwd', 'subflow_fwd_bytes',\n",
        "       'total_length_of_fwd_packets', 'fwd_packet_length_std',\n",
        "       'bwd_packet_length_std', 'flow_iat_mean', 'fwd_iat_min', 'flow_iat_std',\n",
        "       'flow_duration', 'active_mean', 'fwd_psh_flags', 'syn_flag_count',\n",
        "       'fwd_packets_s']\n",
        "\n",
        "df_pwt_infiltration.columns = ['init_win_bytes_fwd', 'subflow_fwd_bytes',\n",
        "       'total_length_of_fwd_packets', 'fwd_packet_length_std',\n",
        "       'bwd_packet_length_std', 'flow_iat_mean', 'fwd_iat_min', 'flow_iat_std',\n",
        "       'flow_duration', 'active_mean', 'fwd_psh_flags', 'syn_flag_count',\n",
        "       'fwd_packets_s']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "R8Annx4eI_Gs",
        "outputId": "d81b255c-b255-4415-faee-981dcbaf186a"
      },
      "source": [
        "df_pwt_benign.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>init_win_bytes_fwd</th>\n",
              "      <th>subflow_fwd_bytes</th>\n",
              "      <th>total_length_of_fwd_packets</th>\n",
              "      <th>fwd_packet_length_std</th>\n",
              "      <th>bwd_packet_length_std</th>\n",
              "      <th>flow_iat_mean</th>\n",
              "      <th>fwd_iat_min</th>\n",
              "      <th>flow_iat_std</th>\n",
              "      <th>flow_duration</th>\n",
              "      <th>active_mean</th>\n",
              "      <th>fwd_psh_flags</th>\n",
              "      <th>syn_flag_count</th>\n",
              "      <th>fwd_packets_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.272716</td>\n",
              "      <td>-0.647020</td>\n",
              "      <td>-0.647020</td>\n",
              "      <td>-0.584707</td>\n",
              "      <td>-0.489364</td>\n",
              "      <td>-1.687959</td>\n",
              "      <td>-0.127557</td>\n",
              "      <td>-1.185899</td>\n",
              "      <td>-1.781634</td>\n",
              "      <td>-0.394253</td>\n",
              "      <td>-0.212966</td>\n",
              "      <td>-0.212966</td>\n",
              "      <td>1.778274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.246321</td>\n",
              "      <td>-0.920325</td>\n",
              "      <td>-0.920325</td>\n",
              "      <td>-0.584707</td>\n",
              "      <td>-0.489364</td>\n",
              "      <td>-0.806958</td>\n",
              "      <td>-1.191152</td>\n",
              "      <td>-1.185899</td>\n",
              "      <td>-0.938415</td>\n",
              "      <td>-0.394253</td>\n",
              "      <td>-0.212966</td>\n",
              "      <td>-0.212966</td>\n",
              "      <td>0.865904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.246321</td>\n",
              "      <td>-0.920325</td>\n",
              "      <td>-0.920325</td>\n",
              "      <td>-0.584707</td>\n",
              "      <td>-0.489364</td>\n",
              "      <td>-0.995693</td>\n",
              "      <td>-1.191152</td>\n",
              "      <td>-1.185899</td>\n",
              "      <td>-1.116171</td>\n",
              "      <td>-0.394253</td>\n",
              "      <td>-0.212966</td>\n",
              "      <td>-0.212966</td>\n",
              "      <td>1.032036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.259953</td>\n",
              "      <td>-0.920325</td>\n",
              "      <td>-0.920325</td>\n",
              "      <td>-0.584707</td>\n",
              "      <td>-0.489364</td>\n",
              "      <td>-1.104276</td>\n",
              "      <td>-1.191152</td>\n",
              "      <td>-1.185899</td>\n",
              "      <td>-1.219155</td>\n",
              "      <td>-0.394253</td>\n",
              "      <td>-0.212966</td>\n",
              "      <td>-0.212966</td>\n",
              "      <td>1.125742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.266436</td>\n",
              "      <td>-0.647020</td>\n",
              "      <td>-0.647020</td>\n",
              "      <td>-0.584707</td>\n",
              "      <td>-0.489364</td>\n",
              "      <td>-1.687959</td>\n",
              "      <td>-0.127557</td>\n",
              "      <td>-1.185899</td>\n",
              "      <td>-1.781634</td>\n",
              "      <td>-0.394253</td>\n",
              "      <td>-0.212966</td>\n",
              "      <td>-0.212966</td>\n",
              "      <td>1.778274</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   init_win_bytes_fwd  subflow_fwd_bytes  ...  syn_flag_count  fwd_packets_s\n",
              "0            0.272716          -0.647020  ...       -0.212966       1.778274\n",
              "1            0.246321          -0.920325  ...       -0.212966       0.865904\n",
              "2            0.246321          -0.920325  ...       -0.212966       1.032036\n",
              "3            0.259953          -0.920325  ...       -0.212966       1.125742\n",
              "4            0.266436          -0.647020  ...       -0.212966       1.778274\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aul55EmNHanT"
      },
      "source": [
        "**Define model architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6C8Z9F8Qqau"
      },
      "source": [
        "# model architecture adapted from https://github.com/ydataai/ydata-synthetic - a model for generating credit card data\n",
        "\n",
        "# import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class GAN():\n",
        "    \n",
        "    def __init__(self, gan_args):\n",
        "        [self.batch_size, lr, self.noise_dim,\n",
        "         self.data_dim, layers_dim] = gan_args\n",
        "\n",
        "        self.generator = Generator(self.batch_size).\\\n",
        "            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)\n",
        "\n",
        "        self.discriminator = Discriminator(self.batch_size).\\\n",
        "            build_model(input_shape=(self.data_dim,), dim=layers_dim)\n",
        "\n",
        "        optimizer = Adam(lr, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "\n",
        "        # The generator takes noise as input and generates samples\n",
        "        z = Input(shape=(self.noise_dim,))\n",
        "        record = self.generator(z)\n",
        "\n",
        "        # For the combined model only the generator is trained\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated samples as input and determines validity\n",
        "        validity = self.discriminator(record)\n",
        "\n",
        "        # The combined model \n",
        "        # Trains the generator to try and create samples that will fool the discriminator \n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def get_data_batch(self, train, batch_size, seed=0):\n",
        "        # # random sampling - some samples will have excessively low or high sampling, but easy to implement\n",
        "        # np.random.seed(seed)\n",
        "        # x = train.loc[ np.random.choice(train.index, batch_size) ].values\n",
        "        # iterate through shuffled indices, so every sample gets covered evenly\n",
        "\n",
        "        start_i = (batch_size * seed) % len(train)\n",
        "        stop_i = start_i + batch_size\n",
        "        shuffle_seed = (batch_size * seed) // len(train)\n",
        "        np.random.seed(shuffle_seed)\n",
        "        train_ix = np.random.choice(list(train.index), replace=False, size=len(train))  # wasteful to shuffle every time\n",
        "        train_ix = list(train_ix) + list(train_ix)  # duplicate to cover ranges past the end of the set\n",
        "        x = train.loc[train_ix[start_i: stop_i]].values\n",
        "        return np.reshape(x, (batch_size, -1))\n",
        "        \n",
        "    def train(self, data, train_arguments):\n",
        "        [cache_prefix, epochs, sample_interval] = train_arguments\n",
        "        \n",
        "        data_cols = data.columns\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((self.batch_size, 1))\n",
        "        fake = np.zeros((self.batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):    \n",
        "            # Discriminator\n",
        "            batch_data = self.get_data_batch(data, self.batch_size)\n",
        "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
        "\n",
        "            # Generate a batch of new samples\n",
        "            gen_data = self.generator.predict(noise)\n",
        "    \n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(batch_data, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    \n",
        "            # Generator\n",
        "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
        "            # Train the generator (to have the discriminator label samples as valid)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "    \n",
        "            # Plot the progress\n",
        "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "    \n",
        "    \n",
        "class Generator():\n",
        "    def __init__(self, batch_size):\n",
        "        self.batch_size=batch_size\n",
        "        \n",
        "    def build_model(self, input_shape, dim, data_dim):\n",
        "        input= Input(shape=input_shape, batch_size=self.batch_size)\n",
        "        x = Dense(dim, activation='relu')(input)\n",
        "        x = Dense(dim * 2, activation='relu')(x)\n",
        "        x = Dense(dim * 4, activation='relu')(x)\n",
        "        x = Dense(data_dim)(x)\n",
        "        return Model(inputs=input, outputs=x)\n",
        "\n",
        "class Discriminator():\n",
        "    def __init__(self,batch_size):\n",
        "        self.batch_size=batch_size\n",
        "    \n",
        "    def build_model(self, input_shape, dim):\n",
        "        input = Input(shape=input_shape, batch_size=self.batch_size)\n",
        "        x = Dense(dim * 4, activation='relu')(input)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(dim * 2, activation='relu')(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(dim, activation='relu')(x)\n",
        "        x = Dense(1, activation='sigmoid')(x)\n",
        "        return Model(inputs=input, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq8nZt93KOVT"
      },
      "source": [
        "**Generating class data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqF72cnzQ1bY"
      },
      "source": [
        "#Define model params and training params\n",
        "\n",
        "# set up dataframe for input into model\n",
        "\n",
        "# EDIT lines 6 & 7 to choose which class of data to generate - change 'df_pwt_class'\n",
        "data_columns = df_pwt_benign.columns\n",
        "df_pwt_benign[data_columns] = df_pwt_benign[data_columns]\n",
        "\n",
        "# set generator parametes\n",
        "batch_size = 512\n",
        "learning_rate=5e-4\n",
        "noise_shape=32\n",
        "input_shape=13\n",
        "dim=128\n",
        " \n",
        "# set training parameters \n",
        "epochs = 2500\n",
        "log_step = 100\n",
        "\n",
        "# assign to variable\n",
        "generator_parameters = [batch_size, learning_rate, noise_shape, input_shape, dim]\n",
        "training_parameters = ['', epochs, log_step]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OKeCCY3mRL9M"
      },
      "source": [
        "# training\n",
        "synthesizer = GAN(generator_parameters)\n",
        "# change passed in dataframe to select which class is to be generated\n",
        "synthesizer.train(df_pwt_benign, training_parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ5HGTL1aqhY"
      },
      "source": [
        "models = {'GAN': ['GAN', False, synthesizer.generator]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVw-S1NiF4ib"
      },
      "source": [
        "np.random.seed(17) # used for retestability \n",
        "\n",
        "noise = np.random.normal(0,1, (1170495, 32)) # number of benign samples to be generated\n",
        "#noise = np.random.normal(0,1, (70302, 32)) # number of dos hulk samples to be generated\n",
        "#noise = np.random.normal(0,1, (2922, 32)) # number of ssh patator samples to be generated\n",
        "#noise = np.random.normal(0,1, (2332, 32)) # number of dos slow loris samples to be generated\n",
        "#noise = np.random.normal(0,1, (11, 32)) # number of heartbleed samples to be generated\n",
        "#noise = np.random.normal(0,1, (4, 32)) # number of infiltration samples to be generated\n",
        "\n",
        "[model_name, with_class, generator_model] = models['GAN']\n",
        "\n",
        "X = generator_model.predict(noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUsnl55amCW-"
      },
      "source": [
        "# Create a dataframe with generated samples, so can be used for evaluation later\n",
        "\n",
        "gen_benign_samples = pd.DataFrame(X, columns=data_columns)\n",
        "# gen_dosHulk_samples = pd.DataFrame(X, columns=data_columns)\n",
        "# gen_sshPatator_samples = pd.DataFrame(X, columns=data_columns)\n",
        "# gen_dosSlowloris_samples = pd.DataFrame(X, columns=data_columns)\n",
        "# gen_heartbleed_samples = pd.DataFrame(X, columns=data_columns)\n",
        "# gen_infiltration_samples = pd.DataFrame(X, columns=data_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBBIH-fpEmXK"
      },
      "source": [
        "gen_benign_samples.to_csv(r'drive/My Drive/Colab/ProjectData/gen_benign_samples.csv')\n",
        "# gen_dosHulk_samples.to_csv(r'drive/My Drive/Colab/ProjectData/gen_dosHulk_samples.csv')\n",
        "# gen_sshPatator_samples.to_csv(r'drive/My Drive/Colab/ProjectData/gen_sshPatator_samples.csv')\n",
        "# gen_dosSlowloris_samples.to_csv(r'drive/My Drive/Colab/ProjectData/gen_dosSlowloris_samples.csv')\n",
        "# gen_heartbleed_samples.to_csv(r'drive/My Drive/Colab/ProjectData/gen_heartbleed_samples.csv')\n",
        "# gen_infiltration_samples.to_csv(r'drive/My Drive/Colab/ProjectData/gen_infiltration_samples.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}